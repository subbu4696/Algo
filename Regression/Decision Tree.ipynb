{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower probability events have more information, higher probability events have less information according to information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/what-is-information-entropy/#:~:text=The%20intuition%20behind%20quantifying%20information,are%20common%20(high%20probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gini impurity\n",
    "GI  = $1 - \\Sigma (p) ^{2}$\n",
    "\n",
    "-  entropy\n",
    "H(s) = $ -p _ {+} \\times log _{2}p_{+} + -p _{-} \\times log _{2}p _{-}$\n",
    "\n",
    "Range of gini impurity is 0-0.5\n",
    "\n",
    "Range of entropy is 0-1\n",
    "\n",
    "Gini Impurity is chosen over entropy as it doesnt involve any log calculation.Computationally Gini impurity is chosen\n",
    "\n",
    "- Information Gain:\n",
    "entropy(parent) - [average entropy of childern]\n",
    "\n",
    "Information gain calculate the reduction in entropy\n",
    "\n",
    "feature is chosen when the imformation gain is more\n",
    "\n",
    "Node impurity/entropy  is measure of homogenity or randomness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impact of outliers :\n",
    "    https://www.quora.com/What-is-the-impact-of-the-outliers-on-the-decision-tree-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 algorithm, stands for Iterative Dichotomiser 3, is a classification algorithm that follows a greedy approach of building a decision tree by selecting a best attribute that yields maximum Information Gain (IG) or minimum Entropy (H)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps in ID3 Algorithm\n",
    "\n",
    "1.Calculate entropy for dataset.\n",
    "\n",
    "2.For each attribute/feature\n",
    "\n",
    "3.Calculate entropy for all its categorical values.\n",
    "\n",
    "4.Calculate information gain for the feature.\n",
    "\n",
    "5.Find the feature with maximum information gain.\n",
    "\n",
    "6.Repeat it until we get the desired tree.\n",
    "\n",
    "https://iq.opengenus.org/id3-algorithm/#:~:text=ID3%20algorithm%2C%20stands%20for%20Iterative,or%20minimum%20Entropy%20(H)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For categorical features impurity of each class is calculated to get the information gain through the split\n",
    "\n",
    "- For numerical futures each value is used as a threshold and impurity is calculated and tree gets split by a threshold of  when the impurity is less. data has to be sorted ascending manner in handling numerical features\n",
    "\n",
    "- Time complexity with numerical features as high as it has to calculate impurity for every number in feature.\n",
    "\n",
    "- compared to linear models Decision tree is robust to outliers as the tree split doesnt take the distance of point in to account as it only split the trees by information gain.\n",
    "\n",
    "- At the end , the result would be the class that is having high probability at the  leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://towardsdatascience.com/decision-trees-d07e0f420175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
