{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower probability events have more information, higher probability events have less information according to information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/what-is-information-entropy/#:~:text=The%20intuition%20behind%20quantifying%20information,are%20common%20(high%20probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gini impurity\n",
    "GI  = $1 - \\Sigma (p) ^{2}$\n",
    "\n",
    "-  entropy\n",
    "H(s) = $ -p _ {+} \\times log _{2}p_{+} + -p _{-} \\times log _{2}p _{-}$\n",
    "\n",
    "Range of gini impurity is 0-0.5\n",
    "\n",
    "Range of entropy is 0-1\n",
    "\n",
    "Gini Impurity is chosen over entropy as it doesnt involve any log calculation.Computationally Gini impurity is chosen\n",
    "\n",
    "- Information Gain:\n",
    "entropy(parent) - [average entropy of childern]\n",
    "\n",
    "Information gain calculate the reduction in entropy\n",
    "\n",
    "feature is chosen when the imformation gain is more\n",
    "\n",
    "Node impurity/entropy  is measure of homogenity or randomness \n",
    "\n",
    "Gain Ratio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impact of outliers :\n",
    "    https://www.quora.com/What-is-the-impact-of-the-outliers-on-the-decision-tree-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 algorithm, stands for Iterative Dichotomiser 3, is a classification algorithm that follows a greedy approach of building a decision tree by selecting a best attribute that yields maximum Information Gain (IG) or minimum Entropy (H)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps in ID3 Algorithm\n",
    "\n",
    "1.Calculate entropy for dataset.\n",
    "\n",
    "2.For each attribute/feature\n",
    "\n",
    "3.Calculate entropy for all its categorical values.\n",
    "\n",
    "4.Calculate information gain for the feature.\n",
    "\n",
    "5.Find the feature with maximum information gain.\n",
    "\n",
    "6.Repeat it until we get the desired tree.\n",
    "\n",
    "https://iq.opengenus.org/id3-algorithm/#:~:text=ID3%20algorithm%2C%20stands%20for%20Iterative,or%20minimum%20Entropy%20(H)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For categorical features impurity of each class is calculated to get the information gain through the split\n",
    "\n",
    "- For numerical futures each value is used as a threshold and impurity is calculated and tree gets split by a threshold of  when the impurity is less. data has to be sorted ascending manner in handling numerical features\n",
    "\n",
    "- Time complexity with numerical features as high as it has to calculate impurity for every number in feature.\n",
    "\n",
    "- compared to linear models Decision tree is robust to outliers as the tree split doesnt take the distance of point in to account as it only split the trees by information gain.\n",
    "\n",
    "- At the end , the result would be the class that is having high probability at the  leaf node.\n",
    "**since trees doesnt add any weights to nominal or ordinal variables , there is no need of one hot encoding to be performed on any variables. We could use just label encoding for both nominal and ordinal** \n",
    "\n",
    "- Class that is having high probability at the leaf node is out put\n",
    "\n",
    "- Stop Criterion:\n",
    "        - can give the depth of trees and threshold impurity for further split and minimum number of samples required for further split.\n",
    "        - Above steps can be performed to avoid overfirtting and decrease the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics\n",
    "- Conditional probability for splitting\n",
    "- Chisquare for dependency between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- techniques to reduce the tre structure\n",
    "    - Early stopping\n",
    "    - Tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost comploexity pruning (post pruning) weakest link pruning\n",
    "\n",
    "ccp alpha is the parameter of sklearn to implement cost complexity pruning\n",
    "\n",
    "classifier tree pruning : based on misclassification\n",
    "https://www.analyticsvidhya.com/blog/2020/10/cost-complexity-pruning-decision-trees\n",
    "\n",
    "https://medium.com/swlh/understanding-decision-trees-f78ec23dffc6\n",
    "\n",
    "regressor tree pruning: based on sum of squared residuals\n",
    "https://www.youtube.com/watch?v=D0efHEJsfHo&t=425s&ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- takes each input of numerical feature as threshold and calculates mean as predicted value at >threshold and < threshold and best split would be the one with less rmse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://towardsdatascience.com/decision-trees-d07e0f420175\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity: There is no problem with collinearity except on further split of features , the features with multi collinearity doesnt add any informaation gain to the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
